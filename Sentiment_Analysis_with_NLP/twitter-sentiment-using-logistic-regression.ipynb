{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e48a618",
   "metadata": {},
   "source": [
    "# Twitter Sentiment analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72206b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources...\n",
      "NLTK setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Error loading omw-eng: Package 'omw-eng' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "print(\"Downloading NLTK resources...\")\n",
    "nltk.download('punkt')           # Tokenizer\n",
    "nltk.download('stopwords')       # Stopwords corpus\n",
    "nltk.download('wordnet')         # WordNet lemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger\n",
    "nltk.download('omw-eng')         # Open Multilingual WordNet\n",
    "\n",
    "\n",
    "# Download the POS tagger with the correct name\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# OR try downloading it with '_eng' suffix\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=False)\n",
    "\n",
    "\n",
    "print(\"NLTK setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35908070",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = ['target','ids','date','flag','user','text']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "df = pd.read_csv(\"D:\\Robotics\\Project Database\\Sentiment analysis Data\\data.1600000.processed.noemoticon.csv\",encoding= DATASET_ENCODING, names=DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b0779f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f46416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "389aa22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['target']=df['target'].replace(4,1)\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb24ec",
   "metadata": {},
   "source": [
    "# Data cleaning --> Text Preparing for the training and testing \n",
    "\n",
    "- ### Steps in Text Cleaning for the project \n",
    "1. Remove urls\n",
    "2. Replave mentions with 'User'\n",
    "3. Remove hashtags but keep the text \n",
    "4. Remove digits\n",
    "5. Remove extra whitespace\n",
    "6. Strip Leading and trailing white spaces \n",
    "7. Remove stopwords \n",
    "8. Tokenize the text\n",
    "9. Lemmatize the text \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b580582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for cleaning the data \n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    stopwordlist = [\n",
    "        'a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "        'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "        'being', 'below', 'between', 'both', 'by', 'can', 'd', 'did', 'do',\n",
    "        'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from',\n",
    "        'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "        'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "        'into', 'is', 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "        'me', 'more', 'most', 'my', 'myself', 'needn', 'no', 'nor', 'now',\n",
    "        'o', 'of', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "        'out', 'own', 're', 's', 'same', 'she', \"shes\", 'should', \"shouldve\", 'so', 'some', 'such',\n",
    "        't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "        'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "        'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was',\n",
    "        'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom',\n",
    "        'why', 'will', 'with', 'won', 'y', 'you', \"youd\", \"youll\", \"youre\",\n",
    "        \"youve\", 'your', 'yours', 'yourself', 'yourselves'\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Function to get NLTK POS tag to WordNEt POS tag\n",
    "\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    # Convert text to lower cases \n",
    "    text = text.lower()\n",
    "   \n",
    "    \n",
    "\n",
    "    # Remove Leading and trailing space\n",
    "    text = text.strip(\" \")\n",
    "   \n",
    "\n",
    "    # Removing the url \n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "   \n",
    "\n",
    "\n",
    "    # Replace @mentions with 'USER'\n",
    "    text = re.sub(r'@[\\S]+','USER',text)\n",
    "    \n",
    "\n",
    "    # Remove hashtags but keep the text \n",
    "    text = re.sub(r'#(\\S+)',' ',text)\n",
    "\n",
    "    # Remove digits \n",
    "    text= re.sub(r'\\d+','',text)\n",
    "\n",
    "    # Remove Extra whitespace \n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "    # Remove stopwrods \n",
    "    text = \" \".join([word for word in text.split() if word not in stopwordlist])\n",
    "\n",
    "    # Tokenize text \n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\[^\\w\\s]')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "    # Part of speecht\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "\n",
    "    # Lemmatize the text \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token ,get_wordnet_pos(tag)) for token , tag in pos_tags]\n",
    "\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d283dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']=df['target'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc672d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']= df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58b7b55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'ids', 'date', 'flag', 'user', 'text'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae7b7e7",
   "metadata": {},
   "source": [
    "## Splitting the data into training and testing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52face7",
   "metadata": {},
   "source": [
    "### First will will split the data to features and target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54954cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = [\"ids\",\"date\",\"flag\",\"user\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f7cb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69730a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features \n",
    "x = df[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2492bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ids                          date      flag             user text\n",
       "0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_     \n",
       "1  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton     \n",
       "2  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus     \n",
       "3  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF     \n",
       "4  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli     "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1962ede8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[target]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf93f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test, y_train , y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b3c7e",
   "metadata": {},
   "source": [
    "### Term frequency and inverse document frequency \n",
    "\n",
    "- used to evaluate the importance of the term wrt to whole document \n",
    "\n",
    "- Term Frequency = No. of times a specific data / word appear in the document \n",
    "- inverse document frequency = Measures how imp is that data/word by considering how rare or common that data / word is \n",
    "\n",
    "- Corpus = [\n",
    "    \"Data analysis with TF-IDF is powerful.\",\n",
    "\n",
    "    \"TF-IDF helps in text preprocessing .\",\n",
    "    \n",
    "    \"Text preprocessing and data analysis are crucial.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfab018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=50000,ngram_range=(1,20))\n",
    "\n",
    "# Fit and transform the training data\n",
    "x_train_vect = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the testing data \n",
    "x_test_vect = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306c0d2",
   "metadata": {},
   "source": [
    "## Model Selection \n",
    "\n",
    "- **We are using Logistic regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4355163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
